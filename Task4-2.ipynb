{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "#delete ids\n",
    "train_data = train_data.drop(columns=[\"Id\"])\n",
    "val_ids = test_data[\"Id\"] #remember for submission\n",
    "test_data = test_data.drop(columns=[\"Id\"])\n",
    "# delete nans\n",
    "for feature in train_data.columns:\n",
    "    percent = train_data[feature].isnull().sum() /  train_data.shape[0]\n",
    "    if (percent > 0.7):\n",
    "        train_data = train_data.drop(columns = feature)\n",
    "        test_data = test_data.drop(columns = feature)\n",
    "# need to convert data\n",
    "for feature in train_data.columns[:-1]:\n",
    "    if (train_data[feature].dtype == 'object'):\n",
    "        train_data[feature] = LabelEncoder().fit_transform(train_data[feature])\n",
    "        test_data[feature] = LabelEncoder().fit_transform(test_data[feature])\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data.drop(columns='SalePrice').values, np.log1p(train_data['SalePrice'].values), test_size = 0.2, random_state = 98987)\n",
    "X_train = SimpleImputer(strategy='most_frequent').fit_transform(X_train)\n",
    "X_test = SimpleImputer(strategy='most_frequent').fit_transform(X_test)\n",
    "# This time we will use Scaler with PCA\n",
    "pca = PCA(n_components = 20)\n",
    "X_train = pca.fit_transform(X_train, y_train)\n",
    "X_test = pca.transform(X_test)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train, y_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гридила миллион лет и недогридила(2 часа где-то) -_-... делаем попроще параметры поиска\n",
    "parameters = {\n",
    "    'num_leaves': [i for i in range(2,128,6)],\n",
    "    'learning_rate': [ 0.001, 0.002, 0.003, 0.004, 0.005, 0.01],\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'n_estimators': [i for i in range(100,6000,100)],\n",
    "    'max_bins':[i for i in range(6,518,64)],}\n",
    "model = GridSearchCV(lgb.LGBMRegressor(), parameters)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Best parameters for LGBM is: {}\".format(model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.01, 'max_bins': 128, 'max_depth': 2, 'n_estimators': 5000, 'num_leaves': 4}\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'num_leaves': [2, 4, 96, 128],\n",
    "    'learning_rate': [ 0.001, 0.005, 0.01],\n",
    "    'max_depth': [2, 4, 6, 8],\n",
    "    'n_estimators': [100, 1000, 5000],\n",
    "    'max_bins':[128,256,512],}\n",
    "lgb_model = GridSearchCV(lgb.LGBMRegressor(), parameters)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "print(\"Best parameters for LGBM is: {}\".format(lgb_model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbsError train: 0.07261147558047464\n",
      "AbsError test: 0.1384169673519346\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMRegressor(**lgb_model.best_params_)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "print('AbsError train:', metrics.mean_absolute_error(lgb_model.predict(X_train), y_train))\n",
    "print('AbsError test:', metrics.mean_absolute_error(lgb_model.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Опять считаем 2 часа и результата нет\n",
    "parameters = {\n",
    "    \"learning_rate\": (0.1, 0.2, 0.3),\n",
    "    \"max_depth\": [ 2, 3, 4, 5, 6],\n",
    "    \"min_child_weight\": [1, 1.5, 2, 4],\n",
    "    \"n_estimators\":[1000, 5000, 10000, 15000],\n",
    "    \"colsample_bytree\":[ 0.2, 0.4, 0.9, 1.]} # reg_lambda??\n",
    "xgb_model = GridSearchCV(xgb.XGBRegressor(), parameters)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"Best parameters for XGB is: {}\".format(xgb_model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.2, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 14400}\n"
     ]
    }
   ],
   "source": [
    "# Уменьшаем кол-во параметров\n",
    "# Попробать с другими значениями \"n_estimators\"\n",
    "# добавить l1 и l2 посмотреть\n",
    "parameters = {\n",
    "    \"learning_rate\": [0.1, 0.2, 0.3],\n",
    "    \"max_depth\": [2, 3, 4, 5, 6],\n",
    "    \"min_child_weight\": [1, 1.5, 2, 4],\n",
    "    \"n_estimators\":[14400],\n",
    "    \"colsample_bytree\":[0.9]} # reg_lambda??\n",
    "xgb_model = GridSearchCV(xgb.XGBRegressor(), parameters)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"Best parameters for XGB is: {}\".format(xgb_model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbsError train: 0.0009239077080119824\n",
      "AbsError test: 0.13990238390245435\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(**xgb_model.best_params_)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print('AbsError train:', metrics.mean_absolute_error(xgb_model.predict(X_train), y_train)) # сильный оверфитинг\n",
    "print('AbsError test:', metrics.mean_absolute_error(xgb_model.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoostRegressor\n",
    "При использовании CatBoostRegressor нет смысла использовать GridSearch \n",
    "поскольку bpdtcnys параметры, которые подходят для нашей задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reinitialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "#delete ids\n",
    "train_data = train_data.drop(columns=[\"Id\"])\n",
    "val_ids = test_data[\"Id\"] #remember for submission\n",
    "test_data = test_data.drop(columns=[\"Id\"])\n",
    "# delete nans\n",
    "for feature in train_data.columns:\n",
    "    percent = train_data[feature].isnull().sum() /  train_data.shape[0]\n",
    "    if (percent > 0.7):\n",
    "        train_data = train_data.drop(columns = feature)\n",
    "        test_data = test_data.drop(columns = feature)\n",
    "# fill nans(Special for cats)\n",
    "train_data = train_data.fillna(train_data.median())\n",
    "test_data = test_data.fillna(test_data.median())\n",
    "# get categorical features list\n",
    "cat_features = []\n",
    "iter = 0\n",
    "for feature in train_data.columns[:-1]:\n",
    "    if (train_data[feature].dtype == 'object'): cat_features.append(iter)\n",
    "    iter +=1\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data.drop(columns='SalePrice').values, np.log1p(train_data['SalePrice'].values), test_size = 0.2, random_state = 98987)\n",
    "X_train = SimpleImputer(strategy='most_frequent').fit_transform(X_train)\n",
    "X_test = SimpleImputer(strategy='most_frequent').fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbsError train: 0.08228153574973592\n",
      "AbsError test: 0.10204139258425525\n",
      "RMSLE: 0.11487674273192189\n",
      "RMSLE: 0.150002504187904\n"
     ]
    }
   ],
   "source": [
    "# Сделать такую же модель\n",
    "# навешать регуляризацию\n",
    "cat_model = CatBoostRegressor(cat_features=cat_features, depth = 2, loss_function = 'RMSE', iterations = 100000, task_type = \"GPU\",  devices = '0:1')\n",
    "train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
    "dev_pool = Pool(X_test, y_test, cat_features=cat_features)\n",
    "cat_model.fit(train_pool, eval_set = dev_pool, early_stopping_rounds = 10, verbose = 0, plot = False)\n",
    "print('AbsError train:', metrics.mean_absolute_error(cat_model.predict(X_train), y_train))\n",
    "print('AbsError test:', metrics.mean_absolute_error(cat_model.predict(X_test), y_test))\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    preds = model.predict(X)\n",
    "    print(\"RMSLE: \" + str(rmsle(preds, y)))\n",
    "evaluate(cat_model, X_train, y_train)\n",
    "evaluate(cat_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomTreeForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'criterion': 'absolute_error', 'max_depth': 1500, 'max_features': 'auto', 'min_samples_split': 5, 'n_estimators': 75}\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'criterion':['squared_error', 'absolute_error', 'poisson'], \n",
    "    'n_estimators':[10,50,75, 100],\n",
    "    'max_features':['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_split':[2,5,9],\n",
    "    'max_depth': [100,500,1000,1500],}\n",
    "rfr_model = GridSearchCV(RandomForestRegressor(), parameters)\n",
    "rfr_model.fit(X_train[:300], y_train[:300])\n",
    "print(\"Best parameters for RFR is: {}\".format(rfr_model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbsError train: 0.12137994057750988\n",
      "AbsError test: 0.16811222302776457\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(**rfr_model.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "print('AbsError train:', metrics.mean_absolute_error(rfr_model.predict(X_train), y_train))\n",
    "print('AbsError test:', metrics.mean_absolute_error(rfr_model.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "362d69d123881ce57a887ab45a3f9acb2a40ab5f881fb7cd46244a03df42c740"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
